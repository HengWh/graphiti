# LLM实体提取计划 (v2)

## 1. 引言：从“确定性”到“概率性”的思维转变

当前基于LLM的实体提取项目已成功搭建了坚实的自动化测试框架，但面临着与传统软件开发迥异的挑战：**结果的概率性与不可预测性**。多次测试失败表明，试图将LLM作为一种“输入-输出”完全固定的确定性函数来使用，是低效且令人沮丧的。

本计划旨在引入一种新的设计哲学：**承认并拥抱LLM的不确定性，并通过系统化、工程化的手段对其进行约束、塑造和验证**。我们的目标不再是追求单次调用的100%正确，而是构建一个**鲁棒、可度量、可迭代**的概率性系统，使其在宏观上达到业务所需的高可靠性。

## 2. 核心改进策略

为了实现上述目标，我们将从以下四个关键领域对现有方案进行系统性升级：

1.  **评估体系升级：从“全有或全无”到“多维洞察”**
    *   **问题**：单一的`deepdiff`精确比对过于严苛，无法区分“格式错误”与“语义错误”，导致对模型核心能力的误判。
    *   **策略**：引入更细粒度的评估指标，包括**格式校验、实体召回率 (Recall)、实体精确率 (Precision) 和 F1-Score**，从而获得对模型性能更全面的理解。

2.  **数据复杂度分级：从“一步到位”到“由易到难”**
    *   **问题**：直接使用长而复杂的对话数据进行测试，使得定位问题根源（是Prompt问题还是上下文处理问题）变得困难。
    *   **策略**：创建不同复杂度（文本长度、实体密度）的测试集，从“单元测试”级别的短文本开始，逐步增加难度，以隔离变量、精准定位性能瓶颈。

3.  **Prompt设计优化：从“单体化”到“分而治之”**
    *   **问题**：一个包罗万象的巨大Prompt会给LLM带来过高的认知负载，导致性能下降、调试困难且扩展性差。
    *   **策略**：采用任务分解思想，探索**按实体类型分解Prompt**或构建**多阶段处理流水线**（如“识别-提取”），降低单步任务的复杂度。

4.  **LLM能力深度应用：从“单次调用”到“反馈闭环”**
    *   **问题**：当前流程仅将LLM作为提取工具，忽略了其强大的逻辑推理和自我修正能力。
    *   **策略**：探索构建**多阶段Agent工作流**（如“提取 -> 自我批判 -> 修正”），让LLM参与到结果的验证和优化中，建立反馈闭环。

## 3. 三阶段优化路线图 (The New Plan)

我们将采用一个循序渐进的三阶段计划来实施上述策略。

### **Phase 1: 建立“可信赖的内核” (Build a "Trusted Core")**

*   **核心目标**: 在最简单的受控场景下，验证核心提取方法并达到近乎完美的性能。
*   **行动步骤**:
    1.  **创建“单元测试集”**:
        *   新建目录 `delphinus/simple_test_cases/`。
        *   从现有数据中，创建**5-10个**短文本（~50-100字）对话样本。
        *   为这些样本创建对应的Ground Truth，且每个样本**只包含 `Person` 和 `Document` 两种实体**。
    2.  **简化Prompt**:
        *   复制 `prompts/extract_entity.py` 为 `prompts/extract_person_document.py`。
        *   修改新Prompt，移除除 `Person` 和 `Document` 之外所有其他实体的定义和指令。
    3.  **升级测试脚本**:
        *   修改 `test_entity_extraction.py`，增加计算 **Precision, Recall, F1-Score** 的逻辑，并在报告中展示。
        *   增加一个命令行参数或配置，使其可以指向新的“单元测试集”和简化版Prompt。
    4.  **迭代调优**:
        *   在此“单元测试集”上反复运行测试，迭代优化简化版Prompt，直到 **F1-Score稳定在95%以上**。

### **Phase 2: 逐步增加复杂度 (Incremental Complexity)**

*   **核心目标**: 测试并提升系统在更真实场景下的鲁棒性。
*   **行动步骤**:
    1.  **增加实体类型**:
        *   在Phase 1验证过的Prompt基础上，**逐一**将 `Task`, `Meeting`, `Project` 等实体加回Prompt和测试集中。
        *   每增加一种实体，就进行一轮完整测试，密切观察其对已有实体F1-Score的影响，并进行相应调整。
    2.  **增加文本长度**:
        *   使用已调优的多实体Prompt，在**中等长度（~200字）**和**完整长度（~500字）**的测试集上运行。
        *   分析并记录性能随文本长度的变化，识别长上下文处理带来的新错误模式。

### **Phase 3: 架构级优化 (Architectural Enhancements)**

*   **核心目标**: 当单一Prompt的性能达到瓶颈时，引入更高级的架构来突破极限。
*   **行动步骤**:
    1.  **识别瓶颈**: 当在Phase 2中发现，增加实体或文本长度导致性能出现不可接受的下降时，即为架构优化的启动信号。
    2.  **实施任务分解**:
        *   **方案A (Prompt Chaining)**: 设计多个更专注的Prompt（如一个负责`Person`/`Meeting`，另一个负责`Document`/`Task`），并串行调用它们，最后合并结果。
        *   **方案B (Self-Critique Loop)**: 设计一个“自我批判”的Prompt，让LLM对第一次的提取结果进行检查和修正，构建两步式Agent工作流。
    3.  **回归测试**: 使用完整的测试用例库（包括所有简单和复杂的case）对新架构进行回归测试，确保其在提升复杂场景性能的同时，没有破坏在简单场景下的高准确率。

---
